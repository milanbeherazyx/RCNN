{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Objectives of Using Selective Search in R-CNN\n",
    "The objectives of using Selective Search in R-CNN are:\n",
    "\n",
    "- **Region Proposal Generation**: To efficiently generate potential bounding box proposals for objects in an image.\n",
    "  \n",
    "- **Balancing Recall and Precision**: To create a balance between detecting as many objects as possible (high recall) while minimizing false positives (high precision).\n",
    "\n",
    "- **Reducing Computational Complexity**: By focusing on a subset of possible object locations, Selective Search significantly reduces the computational load, making the object detection process more efficient.\n",
    "\n",
    "### 2. Phases Involved in R-CNN\n",
    "a. **Region Proposal**: The Selective Search algorithm generates candidate bounding box proposals that likely contain objects within the image. This step identifies regions of interest (RoIs) to be processed further.\n",
    "\n",
    "b. **Warping and Resizing**: The proposed regions are warped and resized to a fixed dimension (e.g., 227x227 pixels) that the pre-trained CNN expects, ensuring uniformity for feature extraction.\n",
    "\n",
    "c. **Pre-trained CNN Architecture**: A CNN (like AlexNet or VGG) pre-trained on a large dataset (such as ImageNet) is used to extract features from the resized RoIs, leveraging learned representations for better accuracy.\n",
    "\n",
    "d. **Pre-trained SVM Models**: Support Vector Machines (SVMs) are trained on the CNN-extracted features to classify the proposed regions as belonging to specific object classes or as background.\n",
    "\n",
    "e. **Clean Up**: This phase involves removing redundant or overlapping bounding boxes based on their confidence scores, ensuring that only the most relevant detections are retained.\n",
    "\n",
    "f. **Implementation of Bounding Box**: Finally, the bounding box coordinates are refined and drawn around detected objects based on the classifier's predictions and the SVM outputs.\n",
    "\n",
    "### 3. Possible Pre-trained CNNs for Pre-trained CNN Architecture\n",
    "Some possible pre-trained CNNs that can be used in the R-CNN architecture include:\n",
    "\n",
    "- **AlexNet**\n",
    "- **VGG16 and VGG19**\n",
    "- **ResNet (e.g., ResNet50)**\n",
    "- **Inception (e.g., InceptionV3)**\n",
    "- **MobileNet**\n",
    "\n",
    "These models provide rich feature representations that enhance the object detection process.\n",
    "\n",
    "### 4. Implementation of SVM in the R-CNN Framework\n",
    "SVM is implemented in the R-CNN framework by:\n",
    "\n",
    "- **Training on Extracted Features**: After feature extraction from the proposed regions using a pre-trained CNN, SVM classifiers are trained using the features extracted from positive (object) and negative (background) examples.\n",
    "\n",
    "- **Classifying Region Proposals**: For each proposed region, the trained SVM predicts the likelihood of it containing a specific object class or being background, effectively classifying the proposals.\n",
    "\n",
    "### 5. How Non-maximum Suppression Works\n",
    "Non-maximum suppression (NMS) is a technique used to eliminate redundant bounding boxes:\n",
    "\n",
    "- **Sorting**: Detected bounding boxes are sorted by their confidence scores.\n",
    "\n",
    "- **Reference Selection**: The box with the highest score is selected as the reference bounding box.\n",
    "\n",
    "- **Overlap Suppression**: Any other boxes with an overlap (measured by Intersection over Union, IoU) above a defined threshold with the reference box are suppressed (removed).\n",
    "\n",
    "- **Iteration**: This process continues until all boxes have been processed, ensuring that only the best bounding boxes for each detected object remain.\n",
    "\n",
    "### 6. How Fast R-CNN is Better than R-CNN\n",
    "Fast R-CNN improves upon R-CNN through several key features:\n",
    "\n",
    "- **Single Training Pipeline**: Fast R-CNN allows for end-to-end training of the entire network, integrating region proposal and classification into a unified framework.\n",
    "\n",
    "- **Shared Convolutional Features**: Instead of processing each proposed region separately, Fast R-CNN processes the entire image once and shares the convolutional features across all proposals, which improves computational efficiency.\n",
    "\n",
    "- **ROI Pooling Layer**: Fast R-CNN introduces an ROI pooling layer that converts varying-size RoIs into fixed-size feature maps, allowing for faster and more effective processing.\n",
    "\n",
    "### 7. ROI Pooling in Fast R-CNN (Mathematical Intuition)\n",
    "ROI pooling in Fast R-CNN serves to convert variable-sized RoIs into fixed-size feature maps:\n",
    "\n",
    "- **Input Feature Map**: Let \\( F \\) be the feature map of size \\( H \\times W \\) and \\( R \\) be the proposed RoI of size \\( r_h \\times r_w \\).\n",
    "\n",
    "- **Scaling**: Each RoI is scaled based on the feature map dimensions, mapping it to the corresponding areas.\n",
    "\n",
    "- **Pooling Operation**: A pooling operation (usually max pooling) is applied over the corresponding regions in the feature map, outputting a fixed-size vector (e.g., \\( 7 \\times 7 \\)), which is then used for classification.\n",
    "\n",
    "### 8. Processes in Fast R-CNN\n",
    "a. **ROI Projection**: The proposed RoIs from the region proposal network (RPN) are projected onto the original feature map, defining their locations in terms of the feature map dimensions.\n",
    "\n",
    "b. **ROI Pooling**: The ROI pooling layer extracts fixed-size feature vectors from the projected RoIs using a pooling operation (like max pooling), enabling consistent input size for subsequent layers.\n",
    "\n",
    "### 9. Change in Object Classifier Activation Function in Fast R-CNN\n",
    "The activation function for the object classifier changed in Fast R-CNN from SVMs to a softmax function to:\n",
    "\n",
    "- **Facilitate End-to-End Training**: This change allows for joint training of the classification and bounding box regression tasks, improving the overall efficiency of the network.\n",
    "\n",
    "- **Probabilistic Outputs**: The softmax function provides probabilities for each class, allowing for a more refined classification approach compared to binary SVM outputs.\n",
    "\n",
    "### 10. Major Changes in Faster R-CNN Compared to Fast R-CNN\n",
    "Faster R-CNN introduces significant improvements over Fast R-CNN:\n",
    "\n",
    "- **Region Proposal Network (RPN)**: Faster R-CNN includes a RPN to generate region proposals directly from the feature maps, eliminating the need for external methods like Selective Search.\n",
    "\n",
    "- **Shared Convolutional Layers**: The RPN and Fast R-CNN share the convolutional layers, allowing for joint training and reducing the overall processing time.\n",
    "\n",
    "- **Improved Speed**: By utilizing RPN, Faster R-CNN speeds up the detection pipeline, making it more suitable for real-time applications.\n",
    "\n",
    "### 11. Concept of Anchor Box\n",
    "Anchor boxes are predefined bounding boxes of various sizes and aspect ratios used in object detection frameworks like Faster R-CNN:\n",
    "\n",
    "- **Multiple Proposals**: Anchor boxes allow the model to predict multiple bounding boxes for each object at different scales and shapes, enhancing detection accuracy.\n",
    "\n",
    "- **Matching with Ground Truth**: During training, anchor boxes are compared with ground truth boxes to determine which anchors should be assigned to specific objects, facilitating better localization and classification.\n",
    "\n",
    "- **Improved Robustness**: Using anchor boxes helps the model generalize better to various object sizes and shapes, resulting in improved performance in diverse detection scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hereâ€™s a high-level guide to implementing Faster R-CNN using the COCO dataset, focusing on the steps you've outlined. For the implementation, you can use popular deep learning frameworks like TensorFlow with Keras or PyTorch. Below is a general overview with code snippets for key steps in PyTorch, which is commonly used for such tasks.\n",
    "\n",
    "### 1. Dataset Preparation\n",
    "\n",
    "#### a. Download and Preprocess the COCO Dataset\n",
    "- **Download** the COCO dataset from the official [COCO website](https://cocodataset.org/#download).\n",
    "- Use `pycocotools` for parsing the annotations and loading images.\n",
    "\n",
    "```bash\n",
    "pip install pycocotools\n",
    "```\n",
    "\n",
    "#### b. Preprocessing\n",
    "You need to preprocess the images and annotations. Here's how to load the dataset using PyTorch:\n",
    "\n",
    "```python\n",
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from pycocotools.coco import COCO\n",
    "from torchvision.datasets import CocoDetection\n",
    "\n",
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Load COCO dataset\n",
    "data_dir = 'path/to/coco'\n",
    "train_coco = CocoDetection(root=os.path.join(data_dir, 'train2017'),\n",
    "                            annFile=os.path.join(data_dir, 'annotations/instances_train2017.json'),\n",
    "                            transform=transform)\n",
    "\n",
    "val_coco = CocoDetection(root=os.path.join(data_dir, 'val2017'),\n",
    "                          annFile=os.path.join(data_dir, 'annotations/instances_val2017.json'),\n",
    "                          transform=transform)\n",
    "```\n",
    "\n",
    "#### c. Splitting the Dataset\n",
    "The COCO dataset is already divided into training and validation sets, so you can use them directly as shown above.\n",
    "\n",
    "### 2. Model Architecture\n",
    "\n",
    "#### a. Build the Faster R-CNN Model\n",
    "You can use the `torchvision` library, which provides a convenient way to implement Faster R-CNN with a pre-trained backbone.\n",
    "\n",
    "```python\n",
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "\n",
    "# Load a pre-trained ResNet backbone\n",
    "backbone = torchvision.models.resnet50(pretrained=True)\n",
    "# Remove the last fully connected layer and the average pooling layer\n",
    "backbone = torch.nn.Sequential(*(list(backbone.children())[:-2]))\n",
    "\n",
    "# Define the anchor generator\n",
    "rpn_anchor_generator = AnchorGenerator(\n",
    "    sizes=((32, 64, 128, 256, 512),),\n",
    "    aspect_ratios=((0.5, 1.0, 2.0),) * 5\n",
    ")\n",
    "\n",
    "# Create the Faster R-CNN model\n",
    "model = FasterRCNN(\n",
    "    backbone,\n",
    "    num_classes=len(train_coco.coco.getCatIds()) + 1,  # +1 for background\n",
    "    rpn_anchor_generator=rpn_anchor_generator\n",
    ")\n",
    "```\n",
    "\n",
    "### 3. Training\n",
    "\n",
    "#### a. Loss Function and Data Augmentation\n",
    "The Faster R-CNN model already incorporates loss functions for classification and regression. You can implement data augmentation as follows:\n",
    "\n",
    "```python\n",
    "import random\n",
    "\n",
    "def random_augment(image, target):\n",
    "    # Random horizontal flipping\n",
    "    if random.random() < 0.5:\n",
    "        image = transforms.functional.hflip(image)\n",
    "        # Adjust target bounding boxes accordingly\n",
    "        # You can implement this as needed\n",
    "    return image, target\n",
    "```\n",
    "\n",
    "#### b. Training Loop\n",
    "Hereâ€™s an example of a basic training loop:\n",
    "\n",
    "```python\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_coco, batch_size=2, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "val_loader = DataLoader(val_coco, batch_size=2, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for images, targets in train_loader:\n",
    "        images = [image.to(device) for image in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        loss_dict = model(images, targets)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(f'Epoch: {epoch}, Loss: {losses.item()}')\n",
    "```\n",
    "\n",
    "### 4. Validation\n",
    "\n",
    "#### a. Evaluate the Model\n",
    "You can calculate mAP (mean Average Precision) using the `pycocotools` library:\n",
    "\n",
    "```python\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Gather predictions\n",
    "# (Implement a method to gather predictions and ground truths for COCOeval)\n",
    "coco_eval = COCOeval(cocoGt, cocoDt)\n",
    "coco_eval.evaluate()\n",
    "coco_eval.accumulate()\n",
    "coco_eval.summarize()\n",
    "```\n",
    "\n",
    "### 5. Inference\n",
    "\n",
    "#### a. Inference Pipeline\n",
    "Hereâ€™s how to implement an inference pipeline:\n",
    "\n",
    "```python\n",
    "def predict(image):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        prediction = model([image.to(device)])\n",
    "    return prediction\n",
    "\n",
    "# Visualize predictions\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "\n",
    "def visualize(image, predictions):\n",
    "    # Draw bounding boxes\n",
    "    boxes = predictions[0]['boxes']\n",
    "    labels = predictions[0]['labels']\n",
    "    scores = predictions[0]['scores']\n",
    "\n",
    "    draw = draw_bounding_boxes(image.cpu(), boxes, labels=labels, colors=\"red\", width=2)\n",
    "    plt.imshow(draw.permute(1, 2, 0).numpy())\n",
    "    plt.show()\n",
    "```\n",
    "\n",
    "### 6. Optional Enhancements\n",
    "\n",
    "#### a. Implement Non-Maximum Suppression (NMS)\n",
    "You can utilize the built-in NMS function in PyTorch:\n",
    "\n",
    "```python\n",
    "from torchvision.ops import nms\n",
    "\n",
    "def apply_nms(predictions, threshold):\n",
    "    boxes = predictions[0]['boxes']\n",
    "    scores = predictions[0]['scores']\n",
    "    keep = nms(boxes, scores, threshold)\n",
    "    return predictions[0]['boxes'][keep], predictions[0]['scores'][keep]\n",
    "```\n",
    "\n",
    "#### b. Fine-Tuning and Experimentation\n",
    "You can experiment with different backbone networks (like VGG or ResNet) and fine-tune the model on the COCO dataset to improve performance.\n",
    "\n",
    "### Conclusion\n",
    "This guide outlines the key steps to implement Faster R-CNN using the COCO dataset in PyTorch. You can expand upon these code snippets and concepts to create a complete object detection pipeline. Remember to test and validate your model thoroughly to ensure optimal performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from pycocotools.coco import COCO\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "\n",
    "# 1. Dataset Preparation\n",
    "# Define data paths\n",
    "data_dir = 'path/to/coco'\n",
    "train_annotations = os.path.join(data_dir, 'annotations/instances_train2017.json')\n",
    "val_annotations = os.path.join(data_dir, 'annotations/instances_val2017.json')\n",
    "\n",
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Load COCO dataset\n",
    "train_coco = torchvision.datasets.CocoDetection(\n",
    "    root=os.path.join(data_dir, 'train2017'),\n",
    "    annFile=train_annotations,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "val_coco = torchvision.datasets.CocoDetection(\n",
    "    root=os.path.join(data_dir, 'val2017'),\n",
    "    annFile=val_annotations,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_coco, batch_size=2, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "val_loader = DataLoader(val_coco, batch_size=2, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "# 2. Model Architecture\n",
    "# Load a pre-trained ResNet backbone\n",
    "backbone = torchvision.models.resnet50(pretrained=True)\n",
    "backbone = torch.nn.Sequential(*(list(backbone.children())[:-2]))\n",
    "\n",
    "# Define the anchor generator\n",
    "rpn_anchor_generator = AnchorGenerator(\n",
    "    sizes=((32, 64, 128, 256, 512),),\n",
    "    aspect_ratios=((0.5, 1.0, 2.0),) * 5\n",
    ")\n",
    "\n",
    "# Create the Faster R-CNN model\n",
    "model = FasterRCNN(\n",
    "    backbone,\n",
    "    num_classes=len(train_coco.coco.getCatIds()) + 1,  # +1 for background\n",
    "    rpn_anchor_generator=rpn_anchor_generator\n",
    ")\n",
    "\n",
    "# Move model to the appropriate device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# 3. Training\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for images, targets in train_loader:\n",
    "        images = [image.to(device) for image in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        loss_dict = model(images, targets)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(f'Epoch: {epoch+1}/{num_epochs}, Loss: {losses.item()}')\n",
    "\n",
    "# 4. Validation\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "cocoGt = COCO(val_annotations)\n",
    "cocoDt = []\n",
    "\n",
    "# Gather predictions\n",
    "for images, targets in val_loader:\n",
    "    images = [image.to(device) for image in images]\n",
    "    with torch.no_grad():\n",
    "        predictions = model(images)\n",
    "\n",
    "    for pred in predictions:\n",
    "        boxes = pred['boxes'].cpu().numpy()\n",
    "        scores = pred['scores'].cpu().numpy()\n",
    "        labels = pred['labels'].cpu().numpy()\n",
    "\n",
    "        for box, score, label in zip(boxes, scores, labels):\n",
    "            cocoDt.append({\n",
    "                'image_id': targets[0]['image_id'].item(),\n",
    "                'category_id': label.item(),\n",
    "                'bbox': box.tolist(),\n",
    "                'score': score.item()\n",
    "            })\n",
    "\n",
    "# Use COCOeval to evaluate\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "cocoDt = cocoGt.loadRes(cocoDt)\n",
    "coco_eval = COCOeval(cocoGt, cocoDt, 'bbox')\n",
    "coco_eval.evaluate()\n",
    "coco_eval.accumulate()\n",
    "coco_eval.summarize()\n",
    "\n",
    "# 5. Inference\n",
    "def predict(image):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        prediction = model([image.to(device)])\n",
    "    return prediction\n",
    "\n",
    "def visualize(image, predictions):\n",
    "    boxes = predictions[0]['boxes']\n",
    "    labels = predictions[0]['labels']\n",
    "    scores = predictions[0]['scores']\n",
    "\n",
    "    # Draw bounding boxes\n",
    "    draw = draw_bounding_boxes(image.cpu(), boxes, labels=labels, colors=\"red\", width=2)\n",
    "    plt.imshow(draw.permute(1, 2, 0).numpy())\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Test on a single image\n",
    "sample_image, _ = val_coco[0]\n",
    "predictions = predict(sample_image)\n",
    "visualize(sample_image, predictions)\n",
    "\n",
    "# 6. Optional Enhancements\n",
    "# Implement Non-Maximum Suppression (NMS)\n",
    "from torchvision.ops import nms\n",
    "\n",
    "def apply_nms(predictions, threshold=0.5):\n",
    "    boxes = predictions[0]['boxes']\n",
    "    scores = predictions[0]['scores']\n",
    "    keep = nms(boxes, scores, threshold)\n",
    "    return predictions[0]['boxes'][keep], predictions[0]['scores'][keep]\n",
    "\n",
    "# Example usage of NMS\n",
    "nms_boxes, nms_scores = apply_nms(predictions)\n",
    "\n",
    "# Visualize NMS results\n",
    "def visualize_nms(image, nms_boxes):\n",
    "    draw = draw_bounding_boxes(image.cpu(), nms_boxes, colors=\"blue\", width=2)\n",
    "    plt.imshow(draw.permute(1, 2, 0).numpy())\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "visualize_nms(sample_image, nms_boxes)\n",
    "\n",
    "# Fine-tuning and experimentation can be done by changing backbone networks and other hyperparameters.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
